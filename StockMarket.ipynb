{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be9eb33a-ce62-4cd9-b52f-7060fe8a1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2021-01-04  222.529999  223.000000  214.809998  217.690002  211.224304   \n",
      "2021-01-05  217.259995  218.520004  215.699997  217.899994  211.428055   \n",
      "2021-01-06  212.169998  216.490005  211.940002  212.250000  205.945862   \n",
      "2021-01-07  214.039993  219.339996  213.710007  218.289993  211.806503   \n",
      "2021-01-08  218.679993  220.580002  217.029999  219.619995  213.096985   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2024-07-09  467.000000  467.329987  458.000000  459.540009  459.540009   \n",
      "2024-07-10  461.220001  466.459991  458.859985  466.250000  466.250000   \n",
      "2024-07-11  462.980011  464.779999  451.549988  454.700012  454.700012   \n",
      "2024-07-12  454.329987  456.359985  450.649994  453.549988  453.549988   \n",
      "2024-07-15  453.420013  457.260010  451.429993  453.959991  453.959991   \n",
      "\n",
      "              Volume  \n",
      "Date                  \n",
      "2021-01-04  37130100  \n",
      "2021-01-05  23823000  \n",
      "2021-01-06  35930700  \n",
      "2021-01-07  27694500  \n",
      "2021-01-08  22956200  \n",
      "...              ...  \n",
      "2024-07-09  17207200  \n",
      "2024-07-10  18196100  \n",
      "2024-07-11  23111200  \n",
      "2024-07-12  16311300  \n",
      "2024-07-15  13621481  \n",
      "\n",
      "[887 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "data={}\n",
    "data1 = yf.download(\"MSFT\", start=\"2021-01-01\")\n",
    "# data2 = yf.download(\"MSFT\")\n",
    "# data3 = yf.download(\"AAPL\", end=\"2023-01-01\")\n",
    "print(data1)\n",
    "with pd.ExcelWriter('All_stocks.xlsx') as writer:\n",
    "    df = pd.DataFrame(data1)\n",
    "    df.to_excel(writer, sheet_name=\"Amazon\")\n",
    "    # df = pd.DataFrame(data2)\n",
    "    # df.to_excel(writer, sheet_name=\"Microsoft\")\n",
    "    # df = pd.DataFrame(data3)\n",
    "    # df.to_excel(writer, sheet_name=\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b52703b0-fb58-4ea1-a252-2c8a15d38ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "         Open      High       Low    Volume\n",
      "730  0.652121  0.663765  0.658245  0.138527\n",
      "390  0.208256  0.200205  0.186639  0.156110\n",
      "118  0.211200  0.206114  0.207865  0.127024\n",
      "440  0.091198  0.106760  0.089973  0.242276\n",
      "309  0.365185  0.358454  0.345913  0.164545\n",
      "..        ...       ...       ...       ...\n",
      "106  0.148373  0.155925  0.149968  0.170857\n",
      "270  0.345564  0.370194  0.326746  0.499124\n",
      "860  0.806969  0.825599  0.809282  0.095868\n",
      "435  0.097634  0.106130  0.098844  0.227672\n",
      "102  0.153279  0.144894  0.138682  0.172508\n",
      "\n",
      "[709 rows x 4 columns]\n",
      "[[0.66761981]\n",
      " [0.18843753]\n",
      " [0.20766907]\n",
      " [0.11158985]\n",
      " [0.35811364]\n",
      " [0.12318357]\n",
      " [0.09090912]\n",
      " [0.30519766]\n",
      " [0.11108065]\n",
      " [0.48067057]\n",
      " [0.22780151]\n",
      " [0.15639808]\n",
      " [0.16654265]\n",
      " [0.13865495]\n",
      " [0.61450792]\n",
      " [0.32815009]\n",
      " [0.79041162]\n",
      " [0.14480436]\n",
      " [0.25130236]\n",
      " [0.36547726]\n",
      " [0.23469513]\n",
      " [0.18553913]\n",
      " [0.6678939 ]\n",
      " [0.20469238]\n",
      " [0.48983588]\n",
      " [0.45595552]\n",
      " [0.30390505]\n",
      " [0.35960205]\n",
      " [0.32427249]\n",
      " [0.20578908]\n",
      " [0.26469777]\n",
      " [0.03579961]\n",
      " [0.28287181]\n",
      " [0.01601973]\n",
      " [0.59124207]\n",
      " [0.32254908]\n",
      " [0.19290275]\n",
      " [0.73193372]\n",
      " [0.14660606]\n",
      " [0.37225338]\n",
      " [0.74419334]\n",
      " [0.30398334]\n",
      " [0.49230349]\n",
      " [0.78841413]\n",
      " [0.17954642]\n",
      " [0.18256232]\n",
      " [0.39798671]\n",
      " [0.        ]\n",
      " [0.30970196]\n",
      " [0.10160198]\n",
      " [0.2750382 ]\n",
      " [0.41525984]\n",
      " [0.33104849]\n",
      " [0.73017122]\n",
      " [0.78175551]\n",
      " [0.85492143]\n",
      " [0.09678433]\n",
      " [0.21013667]\n",
      " [0.28725862]\n",
      " [0.12976384]\n",
      " [0.81935688]\n",
      " [0.32556498]\n",
      " [0.16799185]\n",
      " [0.19689785]\n",
      " [0.63554115]\n",
      " [0.46163482]\n",
      " [0.48642828]\n",
      " [0.06658572]\n",
      " [0.1657201 ]\n",
      " [0.30265164]\n",
      " [0.20449646]\n",
      " [0.09212332]\n",
      " [0.76902588]\n",
      " [0.85041713]\n",
      " [0.32948179]\n",
      " [0.56511692]\n",
      " [0.12874543]\n",
      " [0.26939798]\n",
      " [0.38541379]\n",
      " [0.14558771]\n",
      " [0.16881441]\n",
      " [0.09901688]\n",
      " [0.48948338]\n",
      " [0.84477692]\n",
      " [0.19591865]\n",
      " [0.15565393]\n",
      " [0.16250835]\n",
      " [0.31201286]\n",
      " [0.3329286 ]\n",
      " [0.0515452 ]\n",
      " [0.1380283 ]\n",
      " [0.24198034]\n",
      " [0.7711802 ]\n",
      " [0.12055932]\n",
      " [0.76146647]\n",
      " [0.3381379 ]\n",
      " [0.06118051]\n",
      " [0.76561828]\n",
      " [0.1697544 ]\n",
      " [0.26920219]\n",
      " [0.46327993]\n",
      " [0.12036346]\n",
      " [0.25365246]\n",
      " [0.34260312]\n",
      " [0.12823628]\n",
      " [0.5053464 ]\n",
      " [0.32352829]\n",
      " [0.62296811]\n",
      " [0.27644819]\n",
      " [0.08726646]\n",
      " [0.19819046]\n",
      " [0.30081073]\n",
      " [0.32689668]\n",
      " [0.01049701]\n",
      " [0.65254008]\n",
      " [0.1152716 ]\n",
      " [0.18021233]\n",
      " [0.4426384 ]\n",
      " [0.36488976]\n",
      " [0.06388311]\n",
      " [0.28424272]\n",
      " [0.48047477]\n",
      " [0.4984529 ]\n",
      " [0.8316948 ]\n",
      " [0.19916966]\n",
      " [0.2751949 ]\n",
      " [0.10281619]\n",
      " [0.14002585]\n",
      " [0.46049903]\n",
      " [0.05671535]\n",
      " [0.16857935]\n",
      " [0.16192079]\n",
      " [0.19866046]\n",
      " [0.07868864]\n",
      " [0.91927466]\n",
      " [0.75018605]\n",
      " [0.84454192]\n",
      " [0.11836591]\n",
      " [0.46637424]\n",
      " [0.21307428]\n",
      " [0.12639538]\n",
      " [0.07234343]\n",
      " [0.47130934]\n",
      " [0.81653677]\n",
      " [0.77196349]\n",
      " [0.34996673]\n",
      " [0.10457874]\n",
      " [0.45572052]\n",
      " [0.78880583]\n",
      " [0.10054443]\n",
      " [0.07720029]\n",
      " [0.5594375 ]\n",
      " [0.51079082]\n",
      " [0.1392033 ]\n",
      " [0.11789592]\n",
      " [0.37801109]\n",
      " [0.29642393]\n",
      " [0.16752185]\n",
      " [0.07469354]\n",
      " [0.04982179]\n",
      " [0.34197641]\n",
      " [0.21703029]\n",
      " [0.14233675]\n",
      " [0.51255331]\n",
      " [0.81547928]\n",
      " [0.48098386]\n",
      " [0.82491868]\n",
      " [0.17159531]\n",
      " [0.11930597]\n",
      " [0.15220712]\n",
      " [0.36257886]\n",
      " [0.41623904]\n",
      " [0.49273428]\n",
      " [0.34013551]\n",
      " [0.50526811]\n",
      " [0.28236261]\n",
      " [0.76107477]\n",
      " [0.18929923]\n",
      " [0.17676552]\n",
      " [0.13928165]\n",
      " [0.12580782]\n",
      " [0.13105639]\n",
      " [0.19055264]\n",
      " [0.63381775]\n",
      " [0.12898043]\n",
      " [0.81587086]\n",
      " [0.10477459]\n",
      " [0.30445346]\n",
      " [0.9423838 ]\n",
      " [0.8316165 ]\n",
      " [0.79248752]\n",
      " [0.61658382]\n",
      " [0.17476791]\n",
      " [0.12091181]\n",
      " [0.04735418]\n",
      " [0.80541305]\n",
      " [0.05930046]\n",
      " [0.07579024]\n",
      " [0.26818378]\n",
      " [0.35086752]\n",
      " [0.78880583]\n",
      " [0.19544865]\n",
      " [0.46085153]\n",
      " [0.11045395]\n",
      " [0.34319062]\n",
      " [0.24742475]\n",
      " [0.79515104]\n",
      " [0.37111747]\n",
      " [0.36414556]\n",
      " [0.45489803]\n",
      " [0.50299641]\n",
      " [0.45646473]\n",
      " [0.15851319]\n",
      " [0.39959261]\n",
      " [0.11252985]\n",
      " [0.27233559]\n",
      " [0.15694649]\n",
      " [0.07864949]\n",
      " [0.02365749]\n",
      " [0.15863069]\n",
      " [0.39869182]\n",
      " [0.0528769 ]\n",
      " [0.28792452]\n",
      " [0.65277508]\n",
      " [0.22126039]\n",
      " [0.5053464 ]\n",
      " [0.11264735]\n",
      " [0.08613061]\n",
      " [0.20751248]\n",
      " [0.26740037]\n",
      " [0.21663859]\n",
      " [0.0015667 ]\n",
      " [0.50800992]\n",
      " [0.11656416]\n",
      " [0.24413453]\n",
      " [0.49692529]\n",
      " [0.41553405]\n",
      " [0.77255099]\n",
      " [0.91437866]\n",
      " [0.18056483]\n",
      " [0.2242372 ]\n",
      " [0.63503195]\n",
      " [0.19012185]\n",
      " [0.37957779]\n",
      " [0.51032082]\n",
      " [0.12075517]\n",
      " [0.12706123]\n",
      " [0.09463007]\n",
      " [0.53295996]\n",
      " [0.30739106]\n",
      " [0.32086489]\n",
      " [0.58078415]\n",
      " [0.82578038]\n",
      " [0.30378755]\n",
      " [0.17594303]\n",
      " [0.43077048]\n",
      " [0.16235164]\n",
      " [0.36156045]\n",
      " [0.18111312]\n",
      " [0.82017937]\n",
      " [0.30100665]\n",
      " [0.3867064 ]\n",
      " [0.40981554]\n",
      " [0.20833498]\n",
      " [0.47334616]\n",
      " [0.13309309]\n",
      " [0.62022642]\n",
      " [0.20860918]\n",
      " [0.25615916]\n",
      " [0.64697816]\n",
      " [0.3331636 ]\n",
      " [0.49802199]\n",
      " [0.02130744]\n",
      " [0.95758103]\n",
      " [0.52281545]\n",
      " [0.7259018 ]\n",
      " [0.26826208]\n",
      " [0.21338769]\n",
      " [0.25619836]\n",
      " [0.38760719]\n",
      " [0.39712512]\n",
      " [0.19732876]\n",
      " [0.48717248]\n",
      " [0.46045983]\n",
      " [0.4457326 ]\n",
      " [0.72692021]\n",
      " [0.07708273]\n",
      " [0.34859583]\n",
      " [0.34906583]\n",
      " [0.47252366]\n",
      " [0.28459522]\n",
      " [0.46837174]\n",
      " [0.45293963]\n",
      " [0.81923938]\n",
      " [0.45854063]\n",
      " [0.16011909]\n",
      " [0.33081349]\n",
      " [0.49923619]\n",
      " [0.34170221]\n",
      " [0.18123062]\n",
      " [0.3856097 ]\n",
      " [0.23352002]\n",
      " [0.09745018]\n",
      " [0.18957344]\n",
      " [0.217187  ]\n",
      " [0.11558495]\n",
      " [0.52414715]\n",
      " [0.3836513 ]\n",
      " [0.31581217]\n",
      " [0.9467314 ]\n",
      " [0.63499274]\n",
      " [0.15792569]\n",
      " [0.07676944]\n",
      " [0.42599197]\n",
      " [0.35674273]\n",
      " [0.90211903]\n",
      " [0.14515686]\n",
      " [0.13528649]\n",
      " [0.17496382]\n",
      " [0.46594333]\n",
      " [0.74576004]\n",
      " [0.42932116]\n",
      " [0.34816504]\n",
      " [0.32137397]\n",
      " [0.34538402]\n",
      " [0.18918173]\n",
      " [0.09490422]\n",
      " [0.84963384]\n",
      " [0.30241664]\n",
      " [0.23246252]\n",
      " [1.        ]\n",
      " [0.75077355]\n",
      " [0.2791508 ]\n",
      " [0.73107201]\n",
      " [0.09709762]\n",
      " [0.23888602]\n",
      " [0.83831421]\n",
      " [0.28972622]\n",
      " [0.11476246]\n",
      " [0.27966   ]\n",
      " [0.26759629]\n",
      " [0.28596612]\n",
      " [0.10732049]\n",
      " [0.61893381]\n",
      " [0.44816112]\n",
      " [0.31514626]\n",
      " [0.26313107]\n",
      " [0.09670598]\n",
      " [0.34287732]\n",
      " [0.47373786]\n",
      " [0.62159733]\n",
      " [0.42665776]\n",
      " [0.55050718]\n",
      " [0.69021974]\n",
      " [0.31569467]\n",
      " [0.26642117]\n",
      " [0.15377382]\n",
      " [0.10450038]\n",
      " [0.26508947]\n",
      " [0.29520973]\n",
      " [0.39806512]\n",
      " [0.37758018]\n",
      " [0.85351144]\n",
      " [0.29967495]\n",
      " [0.18479493]\n",
      " [0.49312598]\n",
      " [0.32333237]\n",
      " [0.29336882]\n",
      " [0.39379581]\n",
      " [0.52089613]\n",
      " [0.76013477]\n",
      " [0.46845015]\n",
      " [0.37883358]\n",
      " [0.11668166]\n",
      " [0.93035918]\n",
      " [0.07900199]\n",
      " [0.79969455]\n",
      " [0.30739106]\n",
      " [0.43167127]\n",
      " [0.34452232]\n",
      " [0.08088204]\n",
      " [0.10567544]\n",
      " [0.08836316]\n",
      " [0.79832364]\n",
      " [0.62030471]\n",
      " [0.1095531 ]\n",
      " [0.96756884]\n",
      " [0.10998395]\n",
      " [0.34174142]\n",
      " [0.44522351]\n",
      " [0.49128508]\n",
      " [0.11973682]\n",
      " [0.45489803]\n",
      " [0.11288241]\n",
      " [0.7766637 ]\n",
      " [0.12627788]\n",
      " [0.46351493]\n",
      " [0.07884535]\n",
      " [0.85413814]\n",
      " [0.18804595]\n",
      " [0.1365399 ]\n",
      " [0.79284014]\n",
      " [0.6098077 ]\n",
      " [0.40538953]\n",
      " [0.32109989]\n",
      " [0.63624615]\n",
      " [0.72171091]\n",
      " [0.10704634]\n",
      " [0.78833571]\n",
      " [0.63064514]\n",
      " [0.27229638]\n",
      " [0.79464183]\n",
      " [0.35157264]\n",
      " [0.45047202]\n",
      " [0.29258553]\n",
      " [0.49296928]\n",
      " [0.50248721]\n",
      " [0.47248445]\n",
      " [0.18659675]\n",
      " [0.49218599]\n",
      " [0.48995338]\n",
      " [0.1372841 ]\n",
      " [0.16058909]\n",
      " [0.18859424]\n",
      " [0.36093375]\n",
      " [0.21323099]\n",
      " [0.09298502]\n",
      " [0.29058793]\n",
      " [0.30124165]\n",
      " [0.71556149]\n",
      " [0.43907409]\n",
      " [0.63612865]\n",
      " [0.4952803 ]\n",
      " [0.00301596]\n",
      " [0.7810113 ]\n",
      " [0.06556737]\n",
      " [0.09490422]\n",
      " [0.45732643]\n",
      " [0.32497749]\n",
      " [0.42191846]\n",
      " [0.2790333 ]\n",
      " [0.48940509]\n",
      " [0.60193488]\n",
      " [0.35407933]\n",
      " [0.43527479]\n",
      " [0.29865654]\n",
      " [0.26085937]\n",
      " [0.29799063]\n",
      " [0.8341624 ]\n",
      " [0.74630845]\n",
      " [0.16321334]\n",
      " [0.12992048]\n",
      " [0.29086213]\n",
      " [0.29325133]\n",
      " [0.49163758]\n",
      " [0.82115858]\n",
      " [0.42360266]\n",
      " [0.15788648]\n",
      " [0.00783361]\n",
      " [0.28612271]\n",
      " [0.28103091]\n",
      " [0.22972071]\n",
      " [0.6090635 ]\n",
      " [0.48446987]\n",
      " [0.29082292]\n",
      " [0.81453916]\n",
      " [0.35447104]\n",
      " [0.47561785]\n",
      " [0.35302184]\n",
      " [0.3277193 ]\n",
      " [0.2767616 ]\n",
      " [0.23167911]\n",
      " [0.34115392]\n",
      " [0.3858447 ]\n",
      " [0.09889938]\n",
      " [0.47957387]\n",
      " [0.21871449]\n",
      " [0.75128275]\n",
      " [0.25729507]\n",
      " [0.57671063]\n",
      " [0.27437229]\n",
      " [0.58141085]\n",
      " [0.78363562]\n",
      " [0.83420161]\n",
      " [0.48497908]\n",
      " [0.52038693]\n",
      " [0.84794952]\n",
      " [0.45243042]\n",
      " [0.81931767]\n",
      " [0.48411738]\n",
      " [0.62825583]\n",
      " [0.48493987]\n",
      " [0.1645059 ]\n",
      " [0.94962992]\n",
      " [0.09212332]\n",
      " [0.29861733]\n",
      " [0.79522933]\n",
      " [0.14347266]\n",
      " [0.12604282]\n",
      " [0.18060392]\n",
      " [0.43382559]\n",
      " [0.83142059]\n",
      " [0.48235477]\n",
      " [0.47510865]\n",
      " [0.17324042]\n",
      " [0.91688535]\n",
      " [0.74391914]\n",
      " [0.44628101]\n",
      " [0.80024284]\n",
      " [0.46813674]\n",
      " [0.34174142]\n",
      " [0.45141201]\n",
      " [0.28945202]\n",
      " [0.79381934]\n",
      " [0.2212996 ]\n",
      " [0.40237363]\n",
      " [0.49249928]\n",
      " [0.47087855]\n",
      " [0.34554072]\n",
      " [0.18538243]\n",
      " [0.16384005]\n",
      " [0.49970619]\n",
      " [0.79311422]\n",
      " [0.84113432]\n",
      " [0.11546746]\n",
      " [0.16015824]\n",
      " [0.03940307]\n",
      " [0.52728055]\n",
      " [0.9396812 ]\n",
      " [0.8264855 ]\n",
      " [0.75308445]\n",
      " [0.47514785]\n",
      " [0.64055466]\n",
      " [0.49398769]\n",
      " [0.63855705]\n",
      " [0.24100113]\n",
      " [0.81759427]\n",
      " [0.28929531]\n",
      " [0.14680191]\n",
      " [0.43691978]\n",
      " [0.46574754]\n",
      " [0.36077704]\n",
      " [0.20504487]\n",
      " [0.08734481]\n",
      " [0.14848617]\n",
      " [0.34440482]\n",
      " [0.43351218]\n",
      " [0.67517922]\n",
      " [0.64153386]\n",
      " [0.69358814]\n",
      " [0.13407229]\n",
      " [0.2812267 ]\n",
      " [0.47941716]\n",
      " [0.29983153]\n",
      " [0.42908617]\n",
      " [0.07786614]\n",
      " [0.25047986]\n",
      " [0.75688376]\n",
      " [0.43445229]\n",
      " [0.15530138]\n",
      " [0.24507465]\n",
      " [0.62128392]\n",
      " [0.47072184]\n",
      " [0.02886685]\n",
      " [0.54337869]\n",
      " [0.79609103]\n",
      " [0.44863111]\n",
      " [0.63444445]\n",
      " [0.16411419]\n",
      " [0.64701737]\n",
      " [0.42019506]\n",
      " [0.15459638]\n",
      " [0.64862327]\n",
      " [0.28941293]\n",
      " [0.31812307]\n",
      " [0.1079864 ]\n",
      " [0.09204497]\n",
      " [0.1124907 ]\n",
      " [0.37593518]\n",
      " [0.26830128]\n",
      " [0.12627788]\n",
      " [0.79064662]\n",
      " [0.46151732]\n",
      " [0.15769063]\n",
      " [0.21135088]\n",
      " [0.32250988]\n",
      " [0.19278525]\n",
      " [0.47792875]\n",
      " [0.18244482]\n",
      " [0.38329868]\n",
      " [0.10371703]\n",
      " [0.28941293]\n",
      " [0.14421686]\n",
      " [0.11640751]\n",
      " [0.48603657]\n",
      " [0.25525837]\n",
      " [0.86338175]\n",
      " [0.14065256]\n",
      " [0.14746776]\n",
      " [0.06768242]\n",
      " [0.31659546]\n",
      " [0.05996631]\n",
      " [0.14398181]\n",
      " [0.21992869]\n",
      " [0.272884  ]\n",
      " [0.69413655]\n",
      " [0.46433743]\n",
      " [0.07089423]\n",
      " [0.25498416]\n",
      " [0.63581524]\n",
      " [0.61658382]\n",
      " [0.61728882]\n",
      " [0.26109437]\n",
      " [0.46927264]\n",
      " [0.48595828]\n",
      " [0.62990094]\n",
      " [0.06235556]\n",
      " [0.13708825]\n",
      " [0.99486899]\n",
      " [0.34307312]\n",
      " [0.37272337]\n",
      " [0.35396183]\n",
      " [0.23493013]\n",
      " [0.65159996]\n",
      " [0.26661709]\n",
      " [0.46163482]\n",
      " [0.12839293]\n",
      " [0.77345189]\n",
      " [0.15095372]\n",
      " [0.29368224]\n",
      " [0.20598487]\n",
      " [0.02212994]\n",
      " [0.73914063]\n",
      " [0.24668055]\n",
      " [0.15769063]\n",
      " [0.38329868]\n",
      " [0.0823313 ]\n",
      " [0.7793271 ]\n",
      " [0.06501902]\n",
      " [0.15655478]\n",
      " [0.11699501]\n",
      " [0.12815793]\n",
      " [0.14061335]\n",
      " [0.20496646]\n",
      " [0.29787313]\n",
      " [0.21374019]\n",
      " [0.75751046]\n",
      " [0.44122829]\n",
      " [0.05366024]\n",
      " [0.40233443]\n",
      " [0.40347033]\n",
      " [0.18518663]\n",
      " [0.39642001]\n",
      " [0.35106344]\n",
      " [0.97340485]\n",
      " [0.48439158]\n",
      " [0.8353374 ]\n",
      " [0.14864282]\n",
      " [0.92483646]\n",
      " [0.06599822]\n",
      " [0.77086679]\n",
      " [0.42168346]\n",
      " [0.29348632]\n",
      " [0.26736129]\n",
      " [0.39124991]\n",
      " [0.27754489]\n",
      " [0.76013477]\n",
      " [0.45231292]\n",
      " [0.11672086]\n",
      " [0.45591632]\n",
      " [0.10285533]\n",
      " [0.09004742]\n",
      " [0.11346991]\n",
      " [0.25118486]\n",
      " [0.63174184]\n",
      " [0.47980886]\n",
      " [0.12044182]\n",
      " [0.37624848]\n",
      " [0.09251496]\n",
      " [0.36191295]\n",
      " [0.34742083]\n",
      " [0.31671296]\n",
      " [0.17418041]\n",
      " [0.80321965]\n",
      " [0.24887395]\n",
      " [0.17613882]\n",
      " [0.19141434]\n",
      " [0.75445536]\n",
      " [0.35975875]\n",
      " [0.42939958]\n",
      " [0.25486666]\n",
      " [0.44945361]\n",
      " [0.15369553]\n",
      " [0.20618078]\n",
      " [0.13681404]\n",
      " [0.0567937 ]\n",
      " [0.22705731]\n",
      " [0.48858247]\n",
      " [0.06509731]\n",
      " [0.2211821 ]\n",
      " [0.47800717]\n",
      " [0.10677214]\n",
      " [0.47154446]\n",
      " [0.18992594]\n",
      " [0.16278249]\n",
      " [0.37605268]\n",
      " [0.8294231 ]\n",
      " [0.09870353]\n",
      " [0.13767574]]\n",
      "\n",
      "Testing Data:\n",
      "         Open      High       Low    Volume\n",
      "296  0.257544  0.273598  0.229922  0.479621\n",
      "682  0.460464  0.453356  0.429946  0.150634\n",
      "535  0.185300  0.179562  0.174481  0.256060\n",
      "644  0.503512  0.499606  0.463765  0.374679\n",
      "623  0.469686  0.479199  0.464755  0.186552\n",
      "..        ...       ...       ...       ...\n",
      "25   0.116548  0.119170  0.116585  0.176838\n",
      "84   0.144763  0.137843  0.134168  0.156356\n",
      "10   0.006200  0.009731  0.002732  0.261980\n",
      "442  0.132716  0.142097  0.127356  0.137222\n",
      "762  0.712200  0.707217  0.695707  0.221617\n",
      "\n",
      "[178 rows x 4 columns]\n",
      "[[0.24910895]\n",
      " [0.42505186]\n",
      " [0.17942892]\n",
      " [0.46402414]\n",
      " [0.47910387]\n",
      " [0.08468135]\n",
      " [0.20120636]\n",
      " [0.36488976]\n",
      " [0.64204307]\n",
      " [0.29097963]\n",
      " [0.08613061]\n",
      " [0.45497632]\n",
      " [0.40785714]\n",
      " [0.38075278]\n",
      " [0.29027451]\n",
      " [0.76467828]\n",
      " [0.29285962]\n",
      " [0.30308255]\n",
      " [0.17437621]\n",
      " [0.41032475]\n",
      " [0.71137048]\n",
      " [0.89620461]\n",
      " [0.32391999]\n",
      " [0.17100781]\n",
      " [0.18506913]\n",
      " [0.12514197]\n",
      " [0.37213588]\n",
      " [0.3919157 ]\n",
      " [0.50949821]\n",
      " [0.11730836]\n",
      " [0.17876313]\n",
      " [0.43785989]\n",
      " [0.8287964 ]\n",
      " [0.48450908]\n",
      " [0.94512549]\n",
      " [0.45920642]\n",
      " [0.62774662]\n",
      " [0.39794762]\n",
      " [0.36018954]\n",
      " [0.37554348]\n",
      " [0.16384005]\n",
      " [0.45752222]\n",
      " [0.62073563]\n",
      " [0.15749484]\n",
      " [0.96858725]\n",
      " [0.25141985]\n",
      " [0.15957074]\n",
      " [0.76491328]\n",
      " [0.18209232]\n",
      " [0.08432885]\n",
      " [0.1383808 ]\n",
      " [0.26563788]\n",
      " [0.41240065]\n",
      " [0.40824884]\n",
      " [0.32352829]\n",
      " [0.33896052]\n",
      " [0.6131761 ]\n",
      " [0.51513842]\n",
      " [0.11574166]\n",
      " [0.04966508]\n",
      " [0.13324979]\n",
      " [0.2262739 ]\n",
      " [0.29846075]\n",
      " [0.09227996]\n",
      " [0.60487249]\n",
      " [0.48086636]\n",
      " [0.73162042]\n",
      " [0.74356664]\n",
      " [0.25564995]\n",
      " [0.79452434]\n",
      " [0.20668987]\n",
      " [0.26853628]\n",
      " [0.22353221]\n",
      " [0.89824131]\n",
      " [0.17077281]\n",
      " [0.75210525]\n",
      " [0.35776114]\n",
      " [0.23136582]\n",
      " [0.74540754]\n",
      " [0.47687126]\n",
      " [0.48752498]\n",
      " [0.23516513]\n",
      " [0.39614592]\n",
      " [0.32172659]\n",
      " [0.36261794]\n",
      " [0.15831733]\n",
      " [0.17461121]\n",
      " [0.19153184]\n",
      " [0.32411578]\n",
      " [0.48493987]\n",
      " [0.75974307]\n",
      " [0.11828756]\n",
      " [0.93494189]\n",
      " [0.24472215]\n",
      " [0.2779758 ]\n",
      " [0.45470212]\n",
      " [0.11754336]\n",
      " [0.25619836]\n",
      " [0.34040972]\n",
      " [0.27343229]\n",
      " [0.36250045]\n",
      " [0.29560143]\n",
      " [0.76503078]\n",
      " [0.43652819]\n",
      " [0.03074696]\n",
      " [0.46206573]\n",
      " [0.30323925]\n",
      " [0.45058952]\n",
      " [0.92209477]\n",
      " [0.12894128]\n",
      " [0.47346366]\n",
      " [0.3318319 ]\n",
      " [0.04802003]\n",
      " [0.4977087 ]\n",
      " [0.15044457]\n",
      " [0.08088204]\n",
      " [0.19474365]\n",
      " [0.45250871]\n",
      " [0.41952915]\n",
      " [0.27084718]\n",
      " [0.26270028]\n",
      " [0.0582429 ]\n",
      " [0.7758803 ]\n",
      " [0.1394775 ]\n",
      " [0.29814733]\n",
      " [0.13767574]\n",
      " [0.46754924]\n",
      " [0.23567433]\n",
      " [0.16058909]\n",
      " [0.47068276]\n",
      " [0.78911912]\n",
      " [0.39853512]\n",
      " [0.40405783]\n",
      " [0.36696566]\n",
      " [0.45995062]\n",
      " [0.25141985]\n",
      " [0.09972193]\n",
      " [0.44812191]\n",
      " [0.11879677]\n",
      " [0.16223414]\n",
      " [0.84912464]\n",
      " [0.99482979]\n",
      " [0.34820412]\n",
      " [0.45873642]\n",
      " [0.0871881 ]\n",
      " [0.12353607]\n",
      " [0.43370809]\n",
      " [0.25756927]\n",
      " [0.1100623 ]\n",
      " [0.34444402]\n",
      " [0.17621711]\n",
      " [0.19309854]\n",
      " [0.33191019]\n",
      " [0.32924679]\n",
      " [0.15514473]\n",
      " [0.38059619]\n",
      " [0.25655086]\n",
      " [0.31079866]\n",
      " [0.15475303]\n",
      " [0.53209826]\n",
      " [0.1662293 ]\n",
      " [0.02052409]\n",
      " [0.09921273]\n",
      " [0.50996833]\n",
      " [0.09361166]\n",
      " [0.3863147 ]\n",
      " [0.23598763]\n",
      " [0.15091457]\n",
      " [0.41796245]\n",
      " [0.30214255]\n",
      " [0.8346324 ]\n",
      " [0.12725708]\n",
      " [0.3896831 ]\n",
      " [0.12345778]\n",
      " [0.13403314]\n",
      " [0.01641143]\n",
      " [0.14472601]\n",
      " [0.69726995]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "stock_data1 = pd.read_excel('All_stocks.xlsx', sheet_name='Amazon')\n",
    "# stock_data2 = pd.read_excel('All_stocks.xlsx', sheet_name='Microsoft')\n",
    "# stock_data3 = pd.read_excel('All_stocks.xlsx', sheet_name='Apple')\n",
    "features1 = stock_data1[['Open', 'High', 'Low', 'Volume']]\n",
    "# features2 = stock_data2[['Open', 'High', 'Low', 'Volume']]\n",
    "# features3 = stock_data3[['Open', 'High', 'Low', 'Volume']]\n",
    "\n",
    "# Normalize the data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features1 = scaler.fit_transform(features1)\n",
    "# normalized_features2 = scaler.fit_transform(features2)\n",
    "# normalized_features3 = scaler.fit_transform(features3)\n",
    "\n",
    "\n",
    "\n",
    "# Convert normalized features back to a DataFrame\n",
    "normalized_features_df1 = pd.DataFrame(normalized_features1, columns=features1.columns, index=features1.index)\n",
    "# normalized_features_df2 = pd.DataFrame(normalized_features2, columns=features2.columns, index=features2.index)\n",
    "# normalized_features_df3 = pd.DataFrame(normalized_features3, columns=features3.columns, index=features3.index)\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "target1 = stock_data1['Close']\n",
    "# target2 = stock_data2['Close']\n",
    "# target3 = stock_data3['Close']\n",
    "\n",
    "# Normalize the target variable\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the target variable\n",
    "normalized_target1 = target_scaler.fit_transform(target1.values.reshape(-1, 1))\n",
    "# normalized_target2 = scaler.fit_transform(target2.values.reshape(-1, 1))\n",
    "# normalized_target3 = scaler.fit_transform(target3.values.reshape(-1, 1))\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(normalized_features_df1, normalized_target1, test_size=0.2, random_state=42)\n",
    "# X_train2, X_test2, y_train2, y_test2 = train_test_split(normalized_features_df2, normalized_target2, test_size=0.2, random_state=42)\n",
    "# X_train3, X_test3, y_train3, y_test3 = train_test_split(normalized_features_df3, normalized_target3, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the first few rows of the training and testing sets\n",
    "print(\"Training Data:\")\n",
    "print(X_train1)\n",
    "print(y_train1)\n",
    "print(\"\\nTesting Data:\")\n",
    "print(X_test1)\n",
    "print(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d15fb7d1-4418-4178-b0d5-fcf025e0e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training Data:\")\n",
    "# print(X_train2)\n",
    "# print(y_train2)\n",
    "# print(\"\\nTesting Data:\")\n",
    "# print(X_test2)\n",
    "# print(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "575a5706-8661-412e-9ed7-79fdfadad489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training Data:\")\n",
    "# print(X_train3)\n",
    "# print(y_train3)\n",
    "# print(\"\\nTesting Data:\")\n",
    "# print(X_test3)\n",
    "# print(y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35b8581e-9014-4f81-b166-15747477f324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.1985 - mae: 0.3700 - val_loss: 0.0339 - val_mae: 0.1297\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0319 - mae: 0.1299 - val_loss: 0.0124 - val_mae: 0.1030\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0118 - mae: 0.0924 - val_loss: 0.0029 - val_mae: 0.0458\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0071 - mae: 0.0614 - val_loss: 0.0013 - val_mae: 0.0298\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0057 - mae: 0.0557 - val_loss: 7.8895e-04 - val_mae: 0.0231\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - mae: 0.0499 - val_loss: 5.4099e-04 - val_mae: 0.0178\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - mae: 0.0433 - val_loss: 4.4304e-04 - val_mae: 0.0157\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - mae: 0.0451 - val_loss: 3.2153e-04 - val_mae: 0.0135\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - mae: 0.0432 - val_loss: 3.0286e-04 - val_mae: 0.0131\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0029 - mae: 0.0397 - val_loss: 2.3266e-04 - val_mae: 0.0113\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0026 - mae: 0.0365 - val_loss: 2.7544e-04 - val_mae: 0.0118\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 2.1326e-04 - val_mae: 0.0111\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0026 - mae: 0.0364 - val_loss: 2.1047e-04 - val_mae: 0.0108\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 4.5084e-04 - val_mae: 0.0161\n",
      "Epoch 15/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0022 - mae: 0.0338 - val_loss: 2.1751e-04 - val_mae: 0.0109\n",
      "Epoch 16/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 1.7694e-04 - val_mae: 0.0099\n",
      "Epoch 17/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mae: 0.0299 - val_loss: 1.5953e-04 - val_mae: 0.0094\n",
      "Epoch 18/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0022 - mae: 0.0337 - val_loss: 3.7067e-04 - val_mae: 0.0140\n",
      "Epoch 19/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - mae: 0.0318 - val_loss: 3.2059e-04 - val_mae: 0.0128\n",
      "Epoch 20/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mae: 0.0306 - val_loss: 1.8610e-04 - val_mae: 0.0103\n",
      "Epoch 21/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0022 - mae: 0.0326 - val_loss: 2.5582e-04 - val_mae: 0.0120\n",
      "Epoch 22/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 7.8449e-04 - val_mae: 0.0215\n",
      "Epoch 23/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0018 - mae: 0.0305 - val_loss: 3.5604e-04 - val_mae: 0.0134\n",
      "Epoch 24/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - mae: 0.0303 - val_loss: 3.9236e-04 - val_mae: 0.0145\n",
      "Epoch 25/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0018 - mae: 0.0286 - val_loss: 2.5605e-04 - val_mae: 0.0119\n",
      "Epoch 26/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0016 - mae: 0.0275 - val_loss: 2.7163e-04 - val_mae: 0.0121\n",
      "Epoch 27/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 3.0588e-04 - val_mae: 0.0125\n",
      "Epoch 28/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0012 - mae: 0.0256 - val_loss: 3.5052e-04 - val_mae: 0.0131\n",
      "Epoch 29/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0018 - mae: 0.0287 - val_loss: 2.1906e-04 - val_mae: 0.0111\n",
      "Epoch 30/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 2.5116e-04 - val_mae: 0.0120\n",
      "Epoch 31/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mae: 0.0297 - val_loss: 2.6956e-04 - val_mae: 0.0122\n",
      "Epoch 32/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0016 - mae: 0.0273 - val_loss: 2.2632e-04 - val_mae: 0.0116\n",
      "Epoch 33/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 2.5256e-04 - val_mae: 0.0119\n",
      "Epoch 34/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 2.2283e-04 - val_mae: 0.0111\n",
      "Epoch 35/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 5.3023e-04 - val_mae: 0.0163\n",
      "Epoch 36/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.4220e-04 - mae: 0.0224 - val_loss: 3.8340e-04 - val_mae: 0.0138\n",
      "Epoch 37/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0015 - mae: 0.0259 - val_loss: 4.7394e-04 - val_mae: 0.0156\n",
      "Epoch 38/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0016 - mae: 0.0269 - val_loss: 3.8893e-04 - val_mae: 0.0147\n",
      "Epoch 39/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 3.6088e-04 - val_mae: 0.0140\n",
      "Epoch 40/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 2.2881e-04 - val_mae: 0.0116\n",
      "Epoch 41/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 2.7881e-04 - val_mae: 0.0123\n",
      "Epoch 42/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0010 - mae: 0.0234 - val_loss: 3.4221e-04 - val_mae: 0.0137\n",
      "Epoch 43/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 4.5394e-04 - val_mae: 0.0155\n",
      "Epoch 44/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.8962e-04 - mae: 0.0220 - val_loss: 5.6186e-04 - val_mae: 0.0173\n",
      "Epoch 45/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 3.6565e-04 - val_mae: 0.0141\n",
      "Epoch 46/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9936e-04 - mae: 0.0227 - val_loss: 6.3918e-04 - val_mae: 0.0173\n",
      "Epoch 47/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 6.2537e-04 - val_mae: 0.0174\n",
      "Epoch 48/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.3004e-04 - mae: 0.0210 - val_loss: 4.1377e-04 - val_mae: 0.0145\n",
      "Epoch 49/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.6828e-04 - mae: 0.0231 - val_loss: 3.1430e-04 - val_mae: 0.0129\n",
      "Epoch 50/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 0.0011 - val_mae: 0.0237\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7416e-04 - mae: 0.0239 \n",
      "Test Loss: 0.0009742050897330046, Test MAE: 0.024061765521764755\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Build the MLP model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64, input_dim=X_train1.shape[1], activation='relu'))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history1 = model1.fit(X_train1, y_train1, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss1, mae1 = model1.evaluate(X_test1, y_test1)\n",
    "print(f'Test Loss: {loss1}, Test MAE: {mae1}')\n",
    "\n",
    "\n",
    "# model2 = Sequential()\n",
    "# model2.add(Dense(64, input_dim=X_train2.shape[1], activation='relu'))\n",
    "# model2.add(Dropout(0.2))\n",
    "# model2.add(Dense(32, activation='relu'))\n",
    "# model2.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # Compile the model\n",
    "# model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# # Train the model\n",
    "# history2 = model2.fit(X_train2, y_train2, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss2, mae2 = model2.evaluate(X_test2, y_test2)\n",
    "# print(f'Test Loss: {loss2}, Test MAE: {mae2}')\n",
    "\n",
    "\n",
    "# model3 = Sequential()\n",
    "# model3.add(Dense(64, input_dim=X_train3.shape[1], activation='relu'))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(Dense(32, activation='relu'))\n",
    "# model3.add(Dense(1, activation='linear'))\n",
    "\n",
    "# # Compile the model\n",
    "# model3.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# # Train the model\n",
    "# history3 = model3.fit(X_train3, y_train3, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss3, mae3 = model3.evaluate(X_test3, y_test3)\n",
    "# print(f'Test Loss: {loss3}, Test MAE: {mae3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44c2e8d8-4ea9-42fb-a28a-251573bc1b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Test Mean Squared Error (MSE): 0.0009742051592674863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate and display the Mean Squared Error (MSE)\n",
    "y_pred1 = model1.predict(X_test1)\n",
    "mse1 = mean_squared_error(y_test1, y_pred1)\n",
    "print(f'Test Mean Squared Error (MSE): {mse1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7375498-0404-4350-a958-89fa378d98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted stock price for the next day: 435.086181640625\n"
     ]
    }
   ],
   "source": [
    "# Prepare input for next day prediction\n",
    "# Get the last row of normalized features\n",
    "last_features1 = normalized_features_df1.iloc[-1].values.reshape(1, -1)\n",
    "\n",
    "# Predict the next day's stock price\n",
    "next_day_prediction_normalized1 = model1.predict(last_features1)\n",
    "\n",
    "# Inverse transform the prediction to get it back to the original scale\n",
    "next_day_prediction1 = target_scaler.inverse_transform(next_day_prediction_normalized1)\n",
    "print(f'Predicted stock price for the next day: {next_day_prediction1[0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa0aa6-e6f7-4cb8-a460-8f706c3accd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
